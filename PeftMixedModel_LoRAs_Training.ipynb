{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1937b81d-2de6-4f5d-ac35-eb5c547cff15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:02:01.843342Z",
     "start_time": "2024-12-11T10:01:51.681439Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq\n",
    "from peft import PeftMixedModel, LoraConfig\n",
    "from datasets import load_dataset\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载模型和tokenizer\n",
    "base_model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
    "\n",
    "class CombinedMedicalDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        dataset_name: str,\n",
    "        question_field: str,\n",
    "        answer_field: str,\n",
    "        max_length: int = 512,\n",
    "        split: str = \"train\",\n",
    "        subset_name: Optional[str] = None\n",
    "    ):\n",
    "        if subset_name:\n",
    "            self.dataset = load_dataset(dataset_name, name=subset_name, split=split)\n",
    "        else:\n",
    "            self.dataset = load_dataset(dataset_name, split=split)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.question_field = question_field\n",
    "        self.answer_field = answer_field\n",
    "\n",
    "        self.template = (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "                        \"<|im_start|>user\\n{question}<|im_end|>\\n\"\n",
    "                        \"<|im_start|>assistant\\n{answer}<|im_end|>\\n\")\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, Any]:\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        question = item[self.question_field]\n",
    "        answer = item[self.answer_field]\n",
    "\n",
    "        # 构造问题部分（包括system和user消息）\n",
    "        prompt_text = (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "                      \"<|im_start|>user\\n{}<|im_end|>\\n\"\n",
    "                      \"<|im_start|>assistant\\n\").format(question)\n",
    "\n",
    "        # 构造完整文本\n",
    "        full_text = self.template.format(question=question, answer=answer)\n",
    "\n",
    "        # 编码完整文本，但不做padding\n",
    "        encodings = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None  # 返回列表而不是tensor\n",
    "        )\n",
    "\n",
    "        # 获取prompt部分的长度\n",
    "        prompt_tokens = self.tokenizer(\n",
    "            prompt_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        prompt_length = len(prompt_tokens['input_ids'])\n",
    "\n",
    "        # 获取序列实际长度\n",
    "        sequence_length = len(encodings['input_ids'])\n",
    "\n",
    "        # 创建attention mask和labels\n",
    "        attention_mask = [1] * prompt_length + [1] * (sequence_length - prompt_length)\n",
    "        labels = [-100] * prompt_length + encodings['input_ids'][prompt_length:]\n",
    "\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    处理变长序列的批处理函数，使用模型的pad_token进行padding\n",
    "    \"\"\"\n",
    "    # 找出最长序列的长度\n",
    "    max_length = max([len(x['input_ids']) for x in batch]) + 1\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "\n",
    "    # 对每个样本进行padding\n",
    "    for item in batch:\n",
    "        # 计算需要的padding长度\n",
    "        padding_length = max_length - len(item['input_ids'])\n",
    "\n",
    "        # Padding input_ids，使用pad_token_id\n",
    "        padded_input_ids = item['input_ids'] + [tokenizer.pad_token_id] * padding_length\n",
    "        input_ids.append(padded_input_ids)\n",
    "\n",
    "        # Padding attention_mask，对应位置为0\n",
    "        padded_attention_mask = item['attention_mask'][1:] + [0] * (padding_length + 1)\n",
    "        attention_mask.append(padded_attention_mask)\n",
    "\n",
    "        # Padding labels，对应位置为-100（不计算损失）\n",
    "        padded_labels = item['labels'][1:] + [-100] * (padding_length + 1)\n",
    "        labels.append(padded_labels)\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids),\n",
    "        'attention_mask': torch.tensor(attention_mask),\n",
    "        'labels': torch.tensor(labels)\n",
    "    }\n",
    "\n",
    "def setup_training(batch_size=8):\n",
    "    # 创建数据集\n",
    "    task1_dataset = CombinedMedicalDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_name=\"fzkuji/cMedQA2\",\n",
    "        question_field=\"question\",\n",
    "        answer_field=\"answer\",\n",
    "        max_length=1024,\n",
    "        split=\"train\",\n",
    "        subset_name=\"deduplicate_all\"\n",
    "    )\n",
    "\n",
    "    task2_dataset = CombinedMedicalDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_name=\"fzkuji/HealthCareMagic-100k\",\n",
    "        question_field=\"input\",\n",
    "        answer_field=\"output\",\n",
    "        max_length=1024,\n",
    "        split=\"train\"\n",
    "    )\n",
    "\n",
    "    # 创建数据加载器\n",
    "    task1_loader = DataLoader(\n",
    "        task1_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    task2_loader = DataLoader(\n",
    "        task2_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'tokenizer': tokenizer,\n",
    "        'task1_loader': task1_loader,\n",
    "        'task2_loader': task2_loader,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cfdc98-cc4d-4f55-ab4f-5926db4bddbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:02:14.566419Z",
     "start_time": "2024-12-11T10:02:01.851772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since fzkuji/cMedQA2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'deduplicate_all' at /root/.cache/huggingface/datasets/fzkuji___c_med_qa2/deduplicate_all/0.0.0/2aa94e762ffb3d6cf5d906d1d47d7579b228e2ce (last modified on Sat Dec  7 21:38:51 2024).\n",
      "Using the latest cached version of the dataset since fzkuji/HealthCareMagic-100k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/fzkuji___health_care_magic-100k/default/0.0.0/b2db1a6e13bb0e6cf0077375c50e55639b2aeb39 (last modified on Sat Dec  7 21:39:20 2024).\n"
     ]
    }
   ],
   "source": [
    "# 获取训练设置\n",
    "training_setup = setup_training(batch_size=8)\n",
    "task1_loader = training_setup['task1_loader']\n",
    "task2_loader = training_setup['task2_loader']\n",
    "\n",
    "# 创建数据迭代器\n",
    "task1_iter = iter(task1_loader)\n",
    "task2_iter = iter(task2_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acd6e321-e79f-4061-bbb5-aaaaee12f59d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # 获取一个批次示例\n",
    "# batch = next(task1_iter)\n",
    "# print(\"Batch shapes:\")\n",
    "# for k, v in batch.items():\n",
    "#     print(f\"{k}: {v.shape}\")\n",
    "\n",
    "# print(batch['input_ids'][0])\n",
    "# print(batch['labels'][0])\n",
    "\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# def compute_loss(logits, labels):\n",
    "#     \"\"\"\n",
    "#     计算损失函数\n",
    "#     Args:\n",
    "#         logits: (batch_size, sequence_length, vocab_size)\n",
    "#         labels: (batch_size, sequence_length) with shifted labels\n",
    "#     \"\"\"\n",
    "#     loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "#     flat_logits = logits.view(-1, logits.size(-1))\n",
    "#     flat_labels = labels.view(-1)\n",
    "\n",
    "#     loss = loss_fct(flat_logits, flat_labels)\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "# mixed_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "\n",
    "# mixed_model.train()\n",
    "# mixed_model.to(device)\n",
    "\n",
    "# outputs = mixed_model(\n",
    "#     input_ids=batch['input_ids'].to(device),\n",
    "#     attention_mask=batch['attention_mask'].to(device),\n",
    "#     return_dict=True\n",
    "# )\n",
    "# loss = compute_loss(outputs.logits, batch['labels'].to(device))\n",
    "# loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7785bf-f55c-4b69-b0ff-4b4f0dcb8290",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:04:11.859598Z",
     "start_time": "2024-12-11T10:02:14.619263Z"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "# 定义LoRA配置\n",
    "peft_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 创建PeftMixedModel\n",
    "mixed_model = PeftMixedModel(model, peft_config, adapter_name=\"0\")\n",
    "mixed_model.add_adapter(\"1\", peft_config)\n",
    "mixed_model.set_adapter([\"0\", \"1\"])\n",
    "\n",
    "mixed_model.to(device)\n",
    "\n",
    "# 训练参数\n",
    "train_config = {\n",
    "    'train_steps': 1000,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'lr_start': 1e-4,\n",
    "    'lr_end': 0,\n",
    "    'warmup_steps': 100,\n",
    "    'initial_lr': 0.0\n",
    "}\n",
    "\n",
    "# 创建优化器\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, mixed_model.parameters()),\n",
    "    lr=train_config['initial_lr']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad67acf-6819-437f-8547-f35cf2e6e9ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:39:07.318423Z",
     "start_time": "2024-12-11T10:05:48.665869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global step: 0, Task: 1, Data: task1, Adapter: 0, loss: 3.0251, lr: 0.000000e+00\n",
      "Global step: 10, Task: 3, Data: task2, Adapter: 1, loss: 2.7623, lr: 1.000000e-05\n",
      "Global step: 20, Task: 1, Data: task1, Adapter: 0, loss: 2.9234, lr: 2.000000e-05\n",
      "Global step: 30, Task: 3, Data: task2, Adapter: 1, loss: 2.6990, lr: 3.000000e-05\n",
      "Global step: 40, Task: 1, Data: task1, Adapter: 0, loss: 3.0030, lr: 4.000000e-05\n",
      "Global step: 50, Task: 3, Data: task2, Adapter: 1, loss: 2.6993, lr: 5.000000e-05\n",
      "Global step: 60, Task: 1, Data: task1, Adapter: 0, loss: 2.9024, lr: 6.000000e-05\n",
      "Global step: 70, Task: 3, Data: task2, Adapter: 1, loss: 2.5661, lr: 7.000000e-05\n",
      "Global step: 80, Task: 1, Data: task1, Adapter: 0, loss: 2.7241, lr: 8.000000e-05\n",
      "Global step: 90, Task: 3, Data: task2, Adapter: 1, loss: 2.6528, lr: 9.000000e-05\n",
      "Global step: 100, Task: 1, Data: task1, Adapter: 0, loss: 2.7619, lr: 1.000000e-04\n",
      "Global step: 110, Task: 3, Data: task2, Adapter: 1, loss: 2.4413, lr: 9.998477e-05\n",
      "Global step: 120, Task: 1, Data: task1, Adapter: 0, loss: 2.8328, lr: 9.993908e-05\n",
      "Global step: 130, Task: 3, Data: task2, Adapter: 1, loss: 2.3950, lr: 9.986295e-05\n",
      "Global step: 140, Task: 1, Data: task1, Adapter: 0, loss: 2.8165, lr: 9.975641e-05\n",
      "Global step: 150, Task: 3, Data: task2, Adapter: 1, loss: 2.4958, lr: 9.961947e-05\n",
      "Global step: 160, Task: 1, Data: task1, Adapter: 0, loss: 2.7134, lr: 9.945219e-05\n",
      "Global step: 170, Task: 3, Data: task2, Adapter: 1, loss: 2.4591, lr: 9.925462e-05\n",
      "Global step: 180, Task: 1, Data: task1, Adapter: 0, loss: 2.8607, lr: 9.902681e-05\n",
      "Global step: 190, Task: 3, Data: task2, Adapter: 1, loss: 2.5859, lr: 9.876883e-05\n",
      "Global step: 200, Task: 1, Data: task1, Adapter: 0, loss: 2.8472, lr: 9.848078e-05\n",
      "Global step: 210, Task: 3, Data: task2, Adapter: 1, loss: 2.5959, lr: 9.816272e-05\n",
      "Global step: 220, Task: 1, Data: task1, Adapter: 0, loss: 2.8090, lr: 9.781476e-05\n",
      "Global step: 230, Task: 3, Data: task2, Adapter: 1, loss: 2.4916, lr: 9.743701e-05\n",
      "Global step: 240, Task: 1, Data: task1, Adapter: 0, loss: 2.9674, lr: 9.702957e-05\n",
      "Global step: 250, Task: 3, Data: task2, Adapter: 1, loss: 2.5513, lr: 9.659258e-05\n",
      "Global step: 260, Task: 1, Data: task1, Adapter: 0, loss: 2.7194, lr: 9.612617e-05\n",
      "Global step: 270, Task: 3, Data: task2, Adapter: 1, loss: 2.5229, lr: 9.563048e-05\n",
      "Global step: 280, Task: 1, Data: task1, Adapter: 0, loss: 2.6790, lr: 9.510565e-05\n",
      "Global step: 290, Task: 3, Data: task2, Adapter: 1, loss: 2.4800, lr: 9.455186e-05\n",
      "Global step: 300, Task: 1, Data: task1, Adapter: 0, loss: 2.8637, lr: 9.396926e-05\n",
      "Global step: 310, Task: 3, Data: task2, Adapter: 1, loss: 2.3543, lr: 9.335804e-05\n",
      "Global step: 320, Task: 1, Data: task1, Adapter: 0, loss: 2.9316, lr: 9.271839e-05\n",
      "Global step: 330, Task: 3, Data: task2, Adapter: 1, loss: 2.5122, lr: 9.205049e-05\n",
      "Global step: 340, Task: 1, Data: task1, Adapter: 0, loss: 2.8281, lr: 9.135455e-05\n",
      "Global step: 350, Task: 3, Data: task2, Adapter: 1, loss: 2.5460, lr: 9.063078e-05\n",
      "Global step: 360, Task: 1, Data: task1, Adapter: 0, loss: 2.7682, lr: 8.987940e-05\n",
      "Global step: 370, Task: 3, Data: task2, Adapter: 1, loss: 2.5169, lr: 8.910065e-05\n",
      "Global step: 380, Task: 1, Data: task1, Adapter: 0, loss: 2.6964, lr: 8.829476e-05\n",
      "Global step: 390, Task: 3, Data: task2, Adapter: 1, loss: 2.3869, lr: 8.746197e-05\n",
      "Global step: 400, Task: 1, Data: task1, Adapter: 0, loss: 2.6811, lr: 8.660254e-05\n",
      "Global step: 410, Task: 3, Data: task2, Adapter: 1, loss: 2.5098, lr: 8.571673e-05\n",
      "Global step: 420, Task: 1, Data: task1, Adapter: 0, loss: 2.7981, lr: 8.480481e-05\n",
      "Global step: 430, Task: 3, Data: task2, Adapter: 1, loss: 2.2158, lr: 8.386706e-05\n",
      "Global step: 440, Task: 1, Data: task1, Adapter: 0, loss: 2.8055, lr: 8.290376e-05\n",
      "Global step: 450, Task: 3, Data: task2, Adapter: 1, loss: 2.5160, lr: 8.191520e-05\n",
      "Global step: 460, Task: 1, Data: task1, Adapter: 0, loss: 2.7248, lr: 8.090170e-05\n",
      "Global step: 470, Task: 3, Data: task2, Adapter: 1, loss: 2.3417, lr: 7.986355e-05\n",
      "Global step: 480, Task: 1, Data: task1, Adapter: 0, loss: 2.8122, lr: 7.880108e-05\n",
      "Global step: 490, Task: 3, Data: task2, Adapter: 1, loss: 2.3664, lr: 7.771460e-05\n",
      "Global step: 500, Task: 1, Data: task1, Adapter: 0, loss: 2.8943, lr: 7.660444e-05\n",
      "Global step: 510, Task: 3, Data: task2, Adapter: 1, loss: 2.2193, lr: 7.547096e-05\n",
      "Global step: 520, Task: 1, Data: task1, Adapter: 0, loss: 2.7530, lr: 7.431448e-05\n",
      "Global step: 530, Task: 3, Data: task2, Adapter: 1, loss: 2.4350, lr: 7.313537e-05\n",
      "Global step: 540, Task: 1, Data: task1, Adapter: 0, loss: 2.7821, lr: 7.193398e-05\n",
      "Global step: 550, Task: 3, Data: task2, Adapter: 1, loss: 2.5161, lr: 7.071068e-05\n",
      "Global step: 560, Task: 1, Data: task1, Adapter: 0, loss: 2.5829, lr: 6.946584e-05\n",
      "Global step: 570, Task: 3, Data: task2, Adapter: 1, loss: 2.5945, lr: 6.819984e-05\n",
      "Global step: 580, Task: 1, Data: task1, Adapter: 0, loss: 2.8066, lr: 6.691306e-05\n",
      "Global step: 590, Task: 3, Data: task2, Adapter: 1, loss: 2.4662, lr: 6.560590e-05\n",
      "Global step: 600, Task: 1, Data: task1, Adapter: 0, loss: 2.7853, lr: 6.427876e-05\n",
      "Global step: 610, Task: 3, Data: task2, Adapter: 1, loss: 2.5133, lr: 6.293204e-05\n",
      "Global step: 620, Task: 1, Data: task1, Adapter: 0, loss: 2.6840, lr: 6.156615e-05\n",
      "Global step: 730, Task: 3, Data: task2, Adapter: 1, loss: 2.5082, lr: 4.539905e-05\n",
      "Global step: 740, Task: 1, Data: task1, Adapter: 0, loss: 2.8006, lr: 4.383711e-05\n",
      "Global step: 750, Task: 3, Data: task2, Adapter: 1, loss: 2.6739, lr: 4.226183e-05\n",
      "Global step: 760, Task: 1, Data: task1, Adapter: 0, loss: 2.8361, lr: 4.067366e-05\n",
      "Global step: 770, Task: 3, Data: task2, Adapter: 1, loss: 2.3080, lr: 3.907311e-05\n",
      "Global step: 780, Task: 1, Data: task1, Adapter: 0, loss: 2.7508, lr: 3.746066e-05\n",
      "Global step: 790, Task: 3, Data: task2, Adapter: 1, loss: 2.5317, lr: 3.583679e-05\n",
      "Global step: 800, Task: 1, Data: task1, Adapter: 0, loss: 2.8055, lr: 3.420201e-05\n",
      "Global step: 810, Task: 3, Data: task2, Adapter: 1, loss: 2.5236, lr: 3.255682e-05\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def compute_loss(logits, labels):\n",
    "    \"\"\"\n",
    "    计算损失函数\n",
    "    Args:\n",
    "        logits: (batch_size, sequence_length, vocab_size)\n",
    "        labels: (batch_size, sequence_length) with shifted labels\n",
    "    \"\"\"\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    flat_logits = logits.view(-1, logits.size(-1))\n",
    "    flat_labels = labels.view(-1)\n",
    "    loss = loss_fct(flat_logits, flat_labels)\n",
    "    return loss\n",
    "\n",
    "# 保存第0层 lora_A 第0个 LoRA 参数的初始值\n",
    "lora_param_name = \"base_model.model.model.layers.0.self_attn.q_proj.lora_A.0.weight\"\n",
    "initial_lora_param = mixed_model.state_dict()[lora_param_name].clone()\n",
    "\n",
    "# 创建 GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 初始化计数器\n",
    "accumulation_loss = 0\n",
    "\n",
    "# 设置模型为训练模式\n",
    "mixed_model.train()\n",
    "\n",
    "# 开始训练\n",
    "for step in range(train_config['train_steps']):\n",
    "    # 四种情况轮换(1-4)\n",
    "    current_task = (step % 4) + 1\n",
    "\n",
    "    # 冻结所有LoRA参数\n",
    "    for n, p in mixed_model.named_parameters():\n",
    "        if \"lora\" in n:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # 根据任务设置adapter和可训练参数\n",
    "    if current_task == 1:\n",
    "        mixed_model.set_adapter(\"0\")\n",
    "        for n, p in mixed_model.named_parameters():\n",
    "            p.requires_grad = True if \"0.weight\" in n else False\n",
    "    elif current_task == 2:\n",
    "        mixed_model.set_adapter([\"0\", \"1\"])\n",
    "        for n, p in mixed_model.named_parameters():\n",
    "            p.requires_grad = True if \"0.weight\" in n else False\n",
    "    elif current_task == 3:\n",
    "        mixed_model.set_adapter(\"1\")\n",
    "        for n, p in mixed_model.named_parameters():\n",
    "            p.requires_grad = True if \"1.weight\" in n else False\n",
    "    else:  # current_task == 4\n",
    "        mixed_model.set_adapter([\"0\", \"1\"])\n",
    "        for n, p in mixed_model.named_parameters():\n",
    "            p.requires_grad = True if \"1.weight\" in n else False\n",
    "\n",
    "    for i in range(train_config['gradient_accumulation_steps']):\n",
    "\n",
    "        # 根据任务选择数据集(1,2用task1的数据，3,4用task2的数据)\n",
    "        if current_task <= 2:\n",
    "            try:\n",
    "                batch = next(task1_iter)\n",
    "            except StopIteration:\n",
    "                task1_iter = iter(task1_loader)\n",
    "                batch = next(task1_iter)\n",
    "        else:\n",
    "            try:\n",
    "                batch = next(task2_iter)\n",
    "            except StopIteration:\n",
    "                task2_iter = iter(task2_loader)\n",
    "                batch = next(task2_iter)\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # 动态调节学习率\n",
    "        if step < train_config['warmup_steps']:\n",
    "            current_lr = (step / train_config['warmup_steps']) * train_config['lr_start']\n",
    "        else:\n",
    "            t = (step - train_config['warmup_steps']) / (train_config['train_steps'] - train_config['warmup_steps'])\n",
    "            current_lr = train_config['lr_end'] + (train_config['lr_start'] - train_config['lr_end']) * math.cos(t * math.pi / 2)\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_lr\n",
    "\n",
    "        # 前向传播和损失计算\n",
    "        with autocast():\n",
    "            outputs = mixed_model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                return_dict=True\n",
    "            )\n",
    "            loss = compute_loss(outputs.logits, batch['labels'])\n",
    "\n",
    "        # 累积梯度\n",
    "        loss = loss / train_config['gradient_accumulation_steps']\n",
    "        accumulation_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    # if accumulation_count % train_config['gradient_accumulation_steps'] == 0:\n",
    "    # 执行优化器步骤\n",
    "    scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(mixed_model.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        # 每次执行优化器步骤后，检查参数变化\n",
    "        updated_lora_param = mixed_model.state_dict()[lora_param_name]\n",
    "        # if not torch.equal(initial_lora_param, updated_lora_param):\n",
    "        #     print(f\"[Step {step}] Parameter {lora_param_name} has changed.\")\n",
    "        # else:\n",
    "        #     print(f\"[Step {step}] Parameter {lora_param_name} has NOT changed.\")\n",
    "\n",
    "        # 更新初始值为当前值\n",
    "        initial_lora_param = updated_lora_param.clone()\n",
    "\n",
    "        print(f\"Global step: {step}, Task: {current_task}, Data: {'task1' if current_task <= 2 else 'task2'}, \"\n",
    "              f\"Adapter: {'0' if current_task == 1 else '1' if current_task == 3 else '0+1'}, \"\n",
    "              f\"loss: {accumulation_loss:.4f}, lr: {current_lr:.6e}\")\n",
    "    accumulation_loss = 0\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da345dd-dc69-4d3a-8dfa-041922369f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始评估...\n",
      "开始加载测试数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since fzkuji/cMedQA2 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'deduplicate_all' at /root/.cache/huggingface/datasets/fzkuji___c_med_qa2/deduplicate_all/0.0.0/2aa94e762ffb3d6cf5d906d1d47d7579b228e2ce (last modified on Sat Dec  7 21:38:51 2024).\n",
      "Using the latest cached version of the dataset since fzkuji/HealthCareMagic-100k couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /root/.cache/huggingface/datasets/fzkuji___health_care_magic-100k/default/0.0.0/b2db1a6e13bb0e6cf0077375c50e55639b2aeb39 (last modified on Sat Dec  7 21:39:20 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估 Task1 without LoRA...\n",
      "Evaluated 10 steps, current avg loss: 2.9604\n",
      "Evaluated 20 steps, current avg loss: 3.2198\n",
      "Evaluated 30 steps, current avg loss: 3.0665\n",
      "Evaluated 40 steps, current avg loss: 3.1632\n",
      "Evaluated 50 steps, current avg loss: 3.1793\n",
      "Evaluated 60 steps, current avg loss: 3.2678\n",
      "Evaluated 70 steps, current avg loss: 3.3414\n",
      "Evaluated 80 steps, current avg loss: 3.2962\n",
      "Evaluated 90 steps, current avg loss: 3.2905\n",
      "Evaluated 100 steps, current avg loss: 3.2729\n",
      "Task1 without LoRA - 平均损失: 3.2729\n",
      "\n",
      "评估 Task1 with LoRA0 only...\n",
      "Evaluated 10 steps, current avg loss: 2.6060\n",
      "Evaluated 20 steps, current avg loss: 2.8086\n",
      "Evaluated 30 steps, current avg loss: 2.6910\n",
      "Evaluated 40 steps, current avg loss: 2.7817\n",
      "Evaluated 50 steps, current avg loss: 2.7937\n",
      "Evaluated 60 steps, current avg loss: 2.8220\n",
      "Evaluated 70 steps, current avg loss: 2.8820\n",
      "Evaluated 80 steps, current avg loss: 2.8372\n",
      "Evaluated 90 steps, current avg loss: 2.8460\n",
      "Evaluated 100 steps, current avg loss: 2.8299\n",
      "Task1 with LoRA0 only - 平均损失: 2.8299\n",
      "\n",
      "评估 Task1 with LoRA1 only...\n",
      "Evaluated 10 steps, current avg loss: 3.1844\n",
      "Evaluated 20 steps, current avg loss: 3.4166\n",
      "Evaluated 30 steps, current avg loss: 3.2233\n",
      "Evaluated 40 steps, current avg loss: 3.3098\n",
      "Evaluated 50 steps, current avg loss: 3.3368\n",
      "Evaluated 60 steps, current avg loss: 3.4071\n",
      "Evaluated 70 steps, current avg loss: 3.4783\n",
      "Evaluated 80 steps, current avg loss: 3.4264\n",
      "Evaluated 90 steps, current avg loss: 3.4176\n",
      "Evaluated 100 steps, current avg loss: 3.3959\n",
      "Task1 with LoRA1 only - 平均损失: 3.3959\n",
      "\n",
      "评估 Task1 with both LoRAs...\n",
      "Evaluated 10 steps, current avg loss: 2.6211\n",
      "Evaluated 20 steps, current avg loss: 2.8245\n",
      "Evaluated 30 steps, current avg loss: 2.7031\n",
      "Evaluated 40 steps, current avg loss: 2.7961\n",
      "Evaluated 50 steps, current avg loss: 2.8047\n",
      "Evaluated 60 steps, current avg loss: 2.8295\n",
      "Evaluated 70 steps, current avg loss: 2.8906\n",
      "Evaluated 80 steps, current avg loss: 2.8451\n",
      "Evaluated 90 steps, current avg loss: 2.8532\n",
      "Evaluated 100 steps, current avg loss: 2.8366\n",
      "Task1 with both LoRAs - 平均损失: 2.8366\n",
      "\n",
      "评估 Task2 without LoRA...\n",
      "Evaluated 10 steps, current avg loss: 2.5508\n",
      "Evaluated 20 steps, current avg loss: 2.5464\n",
      "Evaluated 30 steps, current avg loss: 2.6611\n",
      "Evaluated 40 steps, current avg loss: 2.7050\n",
      "Evaluated 50 steps, current avg loss: 2.7463\n",
      "Evaluated 60 steps, current avg loss: 2.7723\n",
      "Evaluated 70 steps, current avg loss: 2.8008\n",
      "Evaluated 80 steps, current avg loss: 2.8436\n",
      "Evaluated 90 steps, current avg loss: 2.8627\n",
      "Evaluated 100 steps, current avg loss: 2.8204\n",
      "Task2 without LoRA - 平均损失: 2.8204\n",
      "\n",
      "评估 Task2 with LoRA0 only...\n",
      "Evaluated 10 steps, current avg loss: 2.7648\n",
      "Evaluated 20 steps, current avg loss: 2.7531\n",
      "Evaluated 30 steps, current avg loss: 2.8625\n",
      "Evaluated 40 steps, current avg loss: 2.9061\n",
      "Evaluated 50 steps, current avg loss: 2.9517\n",
      "Evaluated 60 steps, current avg loss: 2.9765\n",
      "Evaluated 70 steps, current avg loss: 3.0117\n",
      "Evaluated 80 steps, current avg loss: 3.0481\n",
      "Evaluated 90 steps, current avg loss: 3.0652\n",
      "Evaluated 100 steps, current avg loss: 3.0259\n",
      "Task2 with LoRA0 only - 平均损失: 3.0259\n",
      "\n",
      "评估 Task2 with LoRA1 only...\n",
      "Evaluated 10 steps, current avg loss: 2.1606\n",
      "Evaluated 20 steps, current avg loss: 2.1711\n",
      "Evaluated 30 steps, current avg loss: 2.3178\n",
      "Evaluated 40 steps, current avg loss: 2.3450\n",
      "Evaluated 50 steps, current avg loss: 2.3961\n",
      "Evaluated 60 steps, current avg loss: 2.4284\n",
      "Evaluated 70 steps, current avg loss: 2.4574\n",
      "Evaluated 80 steps, current avg loss: 2.4953\n",
      "Evaluated 90 steps, current avg loss: 2.5223\n",
      "Evaluated 100 steps, current avg loss: 2.4836\n",
      "Task2 with LoRA1 only - 平均损失: 2.4836\n",
      "\n",
      "评估 Task2 with both LoRAs...\n",
      "Evaluated 10 steps, current avg loss: 2.1623\n",
      "Evaluated 20 steps, current avg loss: 2.1743\n",
      "Evaluated 30 steps, current avg loss: 2.3233\n",
      "Evaluated 40 steps, current avg loss: 2.3509\n",
      "Evaluated 50 steps, current avg loss: 2.4022\n",
      "Evaluated 60 steps, current avg loss: 2.4327\n",
      "Evaluated 70 steps, current avg loss: 2.4617\n",
      "Evaluated 80 steps, current avg loss: 2.4995\n",
      "Evaluated 90 steps, current avg loss: 2.5264\n",
      "Evaluated 100 steps, current avg loss: 2.4874\n",
      "Task2 with both LoRAs - 平均损失: 2.4874\n",
      "\n",
      "评估结果总结:\n",
      "--------------------------------------------------\n",
      "Task1 without LoRA: 3.2729\n",
      "Task1 with LoRA0 only: 2.8299\n",
      "Task1 with LoRA1 only: 3.3959\n",
      "Task1 with both LoRAs: 2.8366\n",
      "Task2 without LoRA: 2.8204\n",
      "Task2 with LoRA0 only: 3.0259\n",
      "Task2 with LoRA1 only: 2.4836\n",
      "Task2 with both LoRAs: 2.4874\n",
      "\n",
      "生成示例回答:\n",
      "\n",
      "问题: 我最近经常感觉胸口疼痛，这是怎么回事？\n",
      "\n",
      "使用LoRA0的回答:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "我最近经常感觉胸口疼痛，这是怎么回事？\n",
      "assistant\n",
      "考虑是肋间神经痛，需要及时到医院就诊的。\n",
      "\n",
      "使用LoRA1的回答:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "我最近经常感觉胸口疼痛，这是怎么回事？\n",
      "assistant\n",
      "Hello, Thanks for your query. I have gone through your query. The symptoms you described are characteristic of some cardiac problems like heart attack.  You need to get done ECG and coronary angiography if chest pain is not relieved by the above measures. If it is due to heart disease then treatment depends on the cause. You should take a course of medication as per the reports of investigations. In case of cardiac arrest, you should go for immediate life support (CPR). Hope this will help\n",
      "\n",
      "使用两个LoRA的回答:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "我最近经常感觉胸口疼痛，这是怎么回事？\n",
      "assistant\n",
      "你好，根据你的情况考虑是心肌缺血引起的，建议你可以服用硝酸甘油和阿司匹林等药物治疗看看。\n",
      "\n",
      "问题: What should I do if I have a persistent headache?\n",
      "\n",
      "使用LoRA0的回答:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What should I do if I have a persistent headache?\n",
      "assistant\n",
      "头痛的种类很多，如偏头痛、紧张性头痛、丛集性头痛等。治疗原则是尽量找出病因，给予对症处理。平时多注意休息，避免过度疲劳；保持乐观的情绪，适当参加体育锻炼；饮食宜清淡易消化，忌辛辣刺激食物。\n",
      "\n",
      "使用LoRA1的回答:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What should I do if I have a persistent headache?\n",
      "assistant\n",
      "Hello, Thanks for your query, I went through your question and understood your concerns. Persistent headaches can be due to some other causes like stress related, viral infection or neurological disorder. You should consult a neurologist who would run some tests like blood test, CT scan of brain, MRI and may need to run a complete blood count and an electrocardiogram (ECG). If you require any clarification / have doubts / have additional questions, comments / suggestions / ideas please don't hesitate to\n",
      "\n",
      "使用两个LoRA的回答:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "What should I do if I have a persistent headache?\n",
      "assistant\n",
      "Hi, It is very common to experience headaches in the mornings when you wake up and this is usually due to the changes in your sleep cycle. You can try to avoid caffeine in the morning. Also, avoid stress as it can cause headaches. If you are still having trouble with your headache, then I would suggest consulting a doctor. He or she will be able to determine what might be causing the problem. Hope this helps. Best wishes\n"
     ]
    }
   ],
   "source": [
    "def setup_test_data(batch_size=1):\n",
    "    \"\"\"设置测试数据加载器\"\"\"\n",
    "    task1_test_dataset = CombinedMedicalDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_name=\"fzkuji/cMedQA2\",\n",
    "        question_field=\"question\",\n",
    "        answer_field=\"answer\",\n",
    "        max_length=1024,  # 保持与训练时相同\n",
    "        split=\"test\",  # 使用测试集\n",
    "        subset_name=\"deduplicate_all\"\n",
    "    )\n",
    "\n",
    "    task2_test_dataset = CombinedMedicalDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        dataset_name=\"fzkuji/HealthCareMagic-100k\",\n",
    "        question_field=\"input\",\n",
    "        answer_field=\"output\",\n",
    "        max_length=1024,\n",
    "        split=\"train[:1%]\"\n",
    "    )\n",
    "\n",
    "    task1_test_loader = DataLoader(\n",
    "        task1_test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # 测试时不需要打乱\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    task2_test_loader = DataLoader(\n",
    "        task2_test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return task1_test_loader, task2_test_loader\n",
    "\n",
    "def evaluate(model, test_loader, adapter_names, device, max_test_steps=None):\n",
    "    \"\"\"评估函数\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    if adapter_names is not None:\n",
    "        # 设置adapter\n",
    "        model.set_adapter(adapter_names)\n",
    "    else:\n",
    "        model.set_adapter([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if max_test_steps and total_steps >= max_test_steps:\n",
    "                break\n",
    "                \n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    # attention_mask=batch['attention_mask'],\n",
    "                    return_dict=True\n",
    "                )\n",
    "                loss = compute_loss(outputs.logits, batch['labels'])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            \n",
    "            if total_steps % 10 == 0:\n",
    "                print(f\"Evaluated {total_steps} steps, current avg loss: {total_loss/total_steps:.4f}\")\n",
    "    \n",
    "    return total_loss / total_steps\n",
    "\n",
    "def run_evaluation(max_test_steps=100):\n",
    "    \"\"\"运行评估\"\"\"\n",
    "    print(\"开始加载测试数据...\")\n",
    "    task1_test_loader, task2_test_loader = setup_test_data(batch_size=1)\n",
    "    \n",
    "    # 定义测试配置\n",
    "    test_configs = [\n",
    "        {\n",
    "            \"name\": \"Task1 without LoRA\",\n",
    "            \"loader\": task1_test_loader,\n",
    "            \"adapter\": None\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Task1 with LoRA0 only\",\n",
    "            \"loader\": task1_test_loader,\n",
    "            \"adapter\": \"0\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"name\": \"Task1 with LoRA1 only\",\n",
    "        #     \"loader\": task1_test_loader,\n",
    "        #     \"adapter\": [\"1\"]\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"Task1 with both LoRAs\",\n",
    "            \"loader\": task1_test_loader,\n",
    "            \"adapter\": [\"0\", \"1\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Task2 without LoRA\",\n",
    "            \"loader\": task2_test_loader,\n",
    "            \"adapter\": None\n",
    "        },\n",
    "        # {\n",
    "        #     \"name\": \"Task2 with LoRA0 only\",\n",
    "        #     \"loader\": task2_test_loader,\n",
    "        #     \"adapter\": \"0\"\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"Task2 with LoRA1 only\",\n",
    "            \"loader\": task2_test_loader,\n",
    "            \"adapter\": \"1\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Task2 with both LoRAs\",\n",
    "            \"loader\": task2_test_loader,\n",
    "            \"adapter\": [\"0\", \"1\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # 执行评估\n",
    "    for config in test_configs:\n",
    "        print(f\"\\n评估 {config['name']}...\")\n",
    "        loss = evaluate(\n",
    "            model=mixed_model,\n",
    "            test_loader=config['loader'],\n",
    "            adapter_names=config['adapter'],\n",
    "            device=device,\n",
    "            max_test_steps=max_test_steps\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"test_name\": config['name'],\n",
    "            \"loss\": loss\n",
    "        })\n",
    "        print(f\"{config['name']} - 平均损失: {loss:.4f}\")\n",
    "    \n",
    "    # 打印总结果\n",
    "    print(\"\\n评估结果总结:\")\n",
    "    print(\"-\" * 50)\n",
    "    for result in results:\n",
    "        print(f\"{result['test_name']}: {result['loss']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 如果已经训练好了模型，可以加载保存的权重\n",
    "def load_trained_adapters(model, adapter0_path, adapter1_path):\n",
    "    \"\"\"加载训练好的adapter权重\"\"\"\n",
    "    model.load_adapter(adapter0_path, adapter_name=\"0\")\n",
    "    model.load_adapter(adapter1_path, adapter_name=\"1\")\n",
    "    return model\n",
    "\n",
    "# 生成示例函数\n",
    "def generate_example(model, input_text, adapter_names, max_new_tokens=100):\n",
    "    \"\"\"使用模型生成回答\"\"\"\n",
    "    # 准备输入\n",
    "    prompt_text = (\"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
    "                  \"<|im_start|>user\\n{}<|im_end|>\\n\"\n",
    "                  \"<|im_start|>assistant\\n\").format(input_text)\n",
    "    \n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 设置adapter\n",
    "    model.set_adapter(adapter_names)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "    # 1. 如果需要加载已训练的权重：\n",
    "    # mixed_model = load_trained_adapters(\n",
    "    #     mixed_model,\n",
    "    #     adapter0_path=\"path/to/adapter0\",\n",
    "    #     adapter1_path=\"path/to/adapter1\"\n",
    "    # )\n",
    "    \n",
    "# 2. 运行评估\n",
    "print(\"开始评估...\")\n",
    "results = run_evaluation(max_test_steps=100)\n",
    "\n",
    "# 3. 生成一些示例回答\n",
    "print(\"\\n生成示例回答:\")\n",
    "test_questions = [\n",
    "    \"我最近经常感觉胸口疼痛，这是怎么回事？\",\n",
    "    \"What should I do if I have a persistent headache?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(\"\\n问题:\", question)\n",
    "    \n",
    "    print(\"\\n使用LoRA0的回答:\")\n",
    "    response = generate_example(mixed_model, question, \"0\")\n",
    "    print(response)\n",
    "    \n",
    "    print(\"\\n使用LoRA1的回答:\")\n",
    "    response = generate_example(mixed_model, question, \"1\")\n",
    "    print(response)\n",
    "    \n",
    "    print(\"\\n使用两个LoRA的回答:\")\n",
    "    response = generate_example(mixed_model, question, [\"0\", \"1\"])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdcff8-4a23-4dfa-bde5-6fa6ec7ba742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
