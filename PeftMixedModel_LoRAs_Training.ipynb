{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftMixedModel, LoraConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "base_model_name = \"Qwen/Qwen2-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# 定义LoRA配置\n",
    "rank = 4\n",
    "peft_config = LoraConfig(\n",
    "    inference_mode=False,\n",
    "    r=rank,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 创建PeftMixedModel并添加两个adapter: \"0\"和\"1\"\n",
    "mixed_model = PeftMixedModel(model, peft_config, adapter_name=\"0\")\n",
    "mixed_model.add_adapter(\"1\", peft_config)\n",
    "\n",
    "# 冻结基础模型参数，只训练LoRA参数\n",
    "for n, p in mixed_model.named_parameters():\n",
    "    if \"lora\" not in n:\n",
    "        p.requires_grad = False\n",
    "\n",
    "mixed_model.train()\n",
    "mixed_model.to(device)\n",
    "\n",
    "# 加载两个数据集(示例)\n",
    "task1_dataset = load_dataset(\"fzkuji/cMedQA2\", name=\"deduplicate_all\", split=\"train[:1%]\")\n",
    "task2_dataset = load_dataset(\"fzkuji/HealthCareMagic-100k\", split=\"train[:1%]\")\n",
    "\n",
    "# 根据你的数据集字段进行预处理\n",
    "def preprocess(examples):\n",
    "    # 假设数据集中有\"question\"和\"answer\"\n",
    "    inputs = [\"Q: \" + q + \"\\nA: \" + a for q, a in zip(examples[\"question\"], examples[\"answer\"])]\n",
    "    tokenized = tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "task1_dataset = task1_dataset.map(preprocess, batched=True)\n",
    "task2_dataset = task2_dataset.map(preprocess, batched=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch是list，每个元素都是dict\n",
    "    keys = batch[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        out[k] = torch.cat([d[k] for d in batch], dim=0)\n",
    "    return out\n",
    "\n",
    "batch_size = 1\n",
    "task1_loader = DataLoader(task1_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "task2_loader = DataLoader(task2_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 训练参数\n",
    "train_steps = 1000\n",
    "gradient_accumulation_steps = 4\n",
    "lr_start = 1e-4\n",
    "lr_end = 1e-5\n",
    "warmup_steps = 100\n",
    "initial_lr = 0.0  # 第0步学习率为0\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, mixed_model.parameters()), lr=initial_lr)\n",
    "\n",
    "task1_iter = iter(task1_loader)\n",
    "task2_iter = iter(task2_loader)\n",
    "\n",
    "global_step = 0\n",
    "accumulation_count = 0\n",
    "\n",
    "mixed_model.train()\n",
    "\n",
    "for step in range(train_steps):\n",
    "    # 交替从Task1和Task2取数据\n",
    "    if step % 2 == 0:\n",
    "        # Task 1 batch\n",
    "        try:\n",
    "            batch = next(task1_iter)\n",
    "        except StopIteration:\n",
    "            task1_iter = iter(task1_loader)\n",
    "            batch = next(task1_iter)\n",
    "        current_task = 1\n",
    "    else:\n",
    "        # Task 2 batch\n",
    "        try:\n",
    "            batch = next(task2_iter)\n",
    "        except StopIteration:\n",
    "            task2_iter = iter(task2_loader)\n",
    "            batch = next(task2_iter)\n",
    "        current_task = 2\n",
    "\n",
    "    batch = {k: v.to(device) for k,v in batch.items()}\n",
    "\n",
    "    # 动态调节学习率\n",
    "    if step < warmup_steps:\n",
    "        # warmup阶段：0线性上升到lr_start=1e-4\n",
    "        current_lr = (step / warmup_steps) * lr_start\n",
    "    else:\n",
    "        # cosine decay阶段: 从1e-4 到1e-5\n",
    "        t = (step - warmup_steps) / (train_steps - warmup_steps)\n",
    "        # cos(pi/2 * t)从1到0递减\n",
    "        current_lr = lr_end + (lr_start - lr_end) * math.cos(t * math.pi / 2)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "\n",
    "    # 根据当前task设置需要训练的LoRA adapter和冻结策略\n",
    "    for n, p in mixed_model.named_parameters():\n",
    "        if \"lora\" in n:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    if current_task == 1:\n",
    "        # 对Task 1:\n",
    "        # (1) Base+LoRA0\n",
    "        mixed_model.set_adapter(\"0\")\n",
    "        for n,p in mixed_model.named_parameters():\n",
    "            if \"lora0\" in n:\n",
    "                p.requires_grad = True\n",
    "        out_task1_lora0 = mixed_model(**batch)\n",
    "        loss_task1_lora0 = out_task1_lora0.loss\n",
    "\n",
    "        # (2) Base+LoRA0+LoRA1, 此时LoRA1冻结，仅LoRA0训练\n",
    "        mixed_model.set_adapter([\"0\",\"1\"])\n",
    "        for n,p in mixed_model.named_parameters():\n",
    "            if \"lora0\" in n:\n",
    "                p.requires_grad = True\n",
    "            else:\n",
    "                p.requires_grad = False\n",
    "        out_task1_lora0_lora1 = mixed_model(**batch)\n",
    "        loss_task1_lora0_lora1 = out_task1_lora0_lora1.loss\n",
    "\n",
    "        loss = (loss_task1_lora0 + loss_task1_lora0_lora1) / 2.0\n",
    "\n",
    "    else:\n",
    "        # current_task == 2:\n",
    "        # (1) Base+LoRA1\n",
    "        mixed_model.set_adapter(\"1\")\n",
    "        for n,p in mixed_model.named_parameters():\n",
    "            if \"lora1\" in n:\n",
    "                p.requires_grad = True\n",
    "        out_task2_lora1 = mixed_model(**batch)\n",
    "        loss_task2_lora1 = out_task2_lora1.loss\n",
    "\n",
    "        # (2) Base+LoRA0+LoRA1，此时LoRA0冻结，仅LoRA1训练\n",
    "        mixed_model.set_adapter([\"0\",\"1\"])\n",
    "        for n,p in mixed_model.named_parameters():\n",
    "            if \"lora1\" in n:\n",
    "                p.requires_grad = True\n",
    "            else:\n",
    "                p.requires_grad = False\n",
    "        out_task2_lora0_lora1 = mixed_model(**batch)\n",
    "        loss_task2_lora0_lora1 = out_task2_lora0_lora1.loss\n",
    "\n",
    "        loss = (loss_task2_lora1 + loss_task2_lora0_lora1) / 2.0\n",
    "\n",
    "    # 累积梯度\n",
    "    loss = loss / gradient_accumulation_steps\n",
    "    loss.backward()\n",
    "    accumulation_count += 1\n",
    "\n",
    "    if accumulation_count % gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        accumulation_count = 0\n",
    "        global_step += 1\n",
    "        print(f\"Global step: {global_step}, loss: {loss.item() * gradient_accumulation_steps:.4f}, lr: {current_lr:.6e}\")\n",
    "\n",
    "# 训练结束后进行验证测试\n",
    "# 可以在验证和测试时固定adapter，比如使用[\"0\",\"1\"]或单独\"0\"/\"1\"来生成回答，然后计算BLEU或Accuracy\n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
